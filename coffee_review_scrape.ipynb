{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb7998fe",
   "metadata": {},
   "source": [
    "# Coffeereview.com Roast Scraper\n",
    "\n",
    "This notebook is for scraping coffee roast data from https://www.coffeereview.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f8a1945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebb632ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_roast_list(session: requests.Session) -> list[dict]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Scrape coffee reviews from coffeereview.com review list and return a list of data dict.\n",
    "\n",
    "    Args:\n",
    "        session (requests.Session): A requests session object\n",
    "    Returns:\n",
    "        list[dict]: A list of data dicts for all coffee reviews\n",
    "    \"\"\"\n",
    "    \n",
    "    def scrape_page(page_number: int) -> list[dict]:\n",
    "        \"\"\" Return a list of data dicts for a given page number\n",
    "\n",
    "        Args:\n",
    "            page_number (int): The page number to scrape\n",
    "\n",
    "        Returns:\n",
    "            list[dict]: A list of data dicts for a given page number\n",
    "        \"\"\"\n",
    "        page_data = [] \n",
    "        \n",
    "        # This header is required. \n",
    "        headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'}\n",
    "        url = 'https://www.coffeereview.com/advanced-search/page/' + str(page_number) + '/'\n",
    "    \n",
    "        # Send a GET request to the URL\n",
    "        response = session.get(url, headers=headers)\n",
    " \n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the relevant HTML elements and extract the desired data\n",
    "        main = soup.find('main')\n",
    "        results = main.find_all('div', class_='entry-content')\n",
    "        \n",
    "        # Loop through list of entries and extract data \n",
    "        for result in results: \n",
    "            rating = result.find('span', class_='review-template-rating').text\n",
    "\n",
    "            # Extract the coffee roaster name\n",
    "            roaster = result.find('p', class_='review-roaster').a.text\n",
    "\n",
    "            # Extract the coffee name\n",
    "            name = result.find('h2', class_='review-title').a.text\n",
    "\n",
    "            # Extract the review date\n",
    "            review_date = result.find('div', class_='column col-3').strong.next_sibling.strip()\n",
    "\n",
    "            # Extract the price\n",
    "            price_element = soup.find('div', class_='column col-3').find('strong', string='Price:')\n",
    "            price = price_element.next_sibling.strip() if price_element else None\n",
    "\n",
    "            # Extract the review description\n",
    "            description = result.find('div', class_='row row-2').p.string\n",
    "\n",
    "\n",
    "            # Extract the URL for complete review   - TODO\n",
    "            row_3 = result.find('div', class_='row row-3').find_all('div')\n",
    "            try:\n",
    "                complete_review_url = row_3[0].a['href']  \n",
    "            except:\n",
    "                complete_review_url = None\n",
    "\n",
    "            # Extract the URL for the roaster's website\n",
    "            try:\n",
    "                roaster_website_url = row_3[1].a['href']\n",
    "            except:\n",
    "                roaster_website_url = None\n",
    "    \n",
    "            row_data = {\n",
    "                'Rating': rating,\n",
    "                'Roaster': roaster,\n",
    "                'Coffee_Name': name,\n",
    "                'Review_Date': review_date,\n",
    "                'Review_Description': description,\n",
    "                'Complete_Review_URL': complete_review_url,\n",
    "                'Roaster_Website_URL': roaster_website_url\n",
    "                }\n",
    "        \n",
    "            page_data.append(row_data)\n",
    "        \n",
    "        return page_data\n",
    "    \n",
    "    # Initialize list to hold all data\n",
    "    data = []\n",
    "    \n",
    "    # Loop through all pages and scrape data\n",
    "    for i in tqdm(range(0, 121)):\n",
    "        page_data = scrape_page(i)\n",
    "        data.extend(page_data)\n",
    "        \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "081e2c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_roast_page(url: str, session: requests.Session) -> dict:\n",
    "    \"\"\"Scrape the data from a single review page.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the review page.\n",
    "        session (requests.Session): The session object to use for the request.\n",
    "\n",
    "    Returns:\n",
    "        dict: The scraped roast data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # This header is required \n",
    "    headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'}\n",
    "    response = session.get(url=url, headers=headers)\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    def scrape_feature(feature):\n",
    "        \"\"\"Scrape a single feature from the review page.\n",
    "\n",
    "        Args:\n",
    "            feature (str): The feature to scrape.\n",
    "\n",
    "        Returns:\n",
    "            str: The scraped data.\n",
    "        \"\"\"\n",
    "        if soup.find('td', string= feature + ':'):\n",
    "            data = soup.find('td', string=feature + ':').find_next_sibling().text\n",
    "        elif soup.find('h2', string=feature):\n",
    "            data = soup.find('h2', string=feature).find_next_sibling().text\n",
    "        else:\n",
    "            data = None  \n",
    "        return data\n",
    "    \n",
    "    \n",
    "    feature_list = ['Roaster Location',\n",
    "                   'Coffee Origin',\n",
    "                   'Roast Level',\n",
    "                   'Aroma',\n",
    "                   'Acidity/Structure',\n",
    "                   'Acidity',\n",
    "                   'Body',\n",
    "                   'Flavor',\n",
    "                   'Aftertaste',\n",
    "                   'Agtron',\n",
    "                   'Blind Assessment',\n",
    "                   'Notes',\n",
    "                   'Bottom Line',\n",
    "                   'Est. Price']\n",
    "\n",
    "    data = {feature: scrape_feature(feature) for feature in feature_list}\n",
    "    data['url'] = url\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f91adef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [05:18<00:00,  2.63s/it]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "682a8826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abca7a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2420/2420 [19:50<00:00,  2.03it/s]\n"
     ]
    }
   ],
   "source": [
    "with requests.Session() as session:\n",
    "    data = scrape_roast_list(session=session)\n",
    "    \n",
    "    df1 = pd.DataFrame(data)\n",
    "    urls = list(df1['Complete_Review_URL'])\n",
    "\n",
    "    roast_data = []\n",
    "    max_count = len(urls)\n",
    "\n",
    "    for url in tqdm(urls):\n",
    "        roast = scrape_roast_page(url, session)\n",
    "        roast_data.append(roast)\n",
    "        \n",
    "df2 = pd.DataFrame(roast_data)\n",
    "df = df1.merge(df2, left_on=\"Complete_Review_URL\", right_on=\"url\")\n",
    "df.to_csv('data/raw-roast-reviews.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffee-review-env",
   "language": "python",
   "name": "coffee-review-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
